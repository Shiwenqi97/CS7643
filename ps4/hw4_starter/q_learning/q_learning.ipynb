{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning & DQNs (30 points + 5 bonus points)\n",
    "\n",
    "In this section, we will implement a few key parts of the Q-Learning algorithm for two cases - (1) A Q-network which is a single linear layer (referred to in RL literature as \"Q-learning with linear function approximation\") and (2) A deep (convolutional) Q-network, for some Atari game environments where the states are images.\n",
    "\n",
    "Optional Readings: \n",
    "- **Playing Atari with Deep Reinforcement Learning**, Mnih et. al., https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- **The PyTorch DQN Tutorial** https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "\n",
    "Note: ** The bonus credit for this question applies to both sections CS 7643 and CS 4803**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from core.dqn_train import DQNTrain\n",
    "from utils.test_env import EnvTest\n",
    "from utils.schedule import LinearExploration, LinearSchedule\n",
    "from utils.preprocess import greyscale\n",
    "from utils.wrappers import PreproWrapper, MaxAndSkipEnv\n",
    "\n",
    "from linear_qnet import LinearQNet\n",
    "from cnn_qnet import ConvQNet\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', 0)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup Q-Learning with Linear Function Approximation\n",
    "\n",
    "Training Q-networks using (Deep) Q-learning involves a lot of moving parts. However, for this assignment, the scaffolding for the first 3 points listed below is provided in full and you must only complete point 4. You may skip to point 4 if you only care about the implementation required for this assignment.\n",
    "\n",
    "1. **Environments**: We will use the standardized OpenAI Gym framework for environment API calls (read through http://gym.openai.com/docs/ if you want to know more details about this interface). Specifically, we will use a custom Test environment defined in `utils/test_env.py` for initial sanity checks and then Gym-Atari environments later on.\n",
    "\n",
    "\n",
    "2. **Exploration**: In order to train any RL model, we require experience or \"data\" gathered from interacting with the environment by taking actions. What policy should we use to collect this experience? Given a Q-network, one may be tempted to define a greedy policy which always picks the highest valued action at every state. However, this strategy will in most cases not work since we may get stuck in a local minima and never explore new states in the environment which may lead to a better reward. Hence, for the purpose of gathering experience (or \"data\") from the environment, it is useful to follow a policy that deviates from the greedy policy slightly in order to explore new states. A common strategy used in RL is to follow an $\\epsilon$-greedy policy which with probability $0 < \\epsilon < 1$ picks a random action instead of the action provided by the greedy policy.\n",
    "\n",
    "\n",
    "3. **Replay Buffers**: Data gathered from a single trajectory of states and actions in the environment provides us with a batch of highly correlated (non IID) data, which leads to high variance in gradient updates and convergence. In order to ameliorate this, replay buffers are used to gather a set of transitions i.e. (state, action, reward, next state) tuples, by executing multiple trajectories in the environment. Now, for updating the Q-Network, we will first wait to fill up our replay buffer with a sufficiently large number of transitions over multiple different trajectories, and then randomly sample a batch of transitions to compute loss and update the models.\n",
    "\n",
    "\n",
    "4. **Q-Learning network, loss and update**: Finally, we come to the part of Q-learning that we will implement for this assignment -- the Q-network, loss function and update. In particular, we will implement a variant of Q-Learning called \"Double Q-Learning\", where we will maintain two Q networks -- the first Q network is used to pick actions and the second \"target\" Q network is used to compute Q-values for the picked actions. Here is some referance material on the same - [Blog 1](https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3), [Blog 2](https://medium.com/@ameetsd97/deep-double-q-learning-why-you-should-use-it-bedf660d5295), but we will not need to get into the details of Double Q-learning for this assignment. Now, let's walk through the steps required to implement this below.\n",
    "\n",
    "    - **Linear Q-Network**: In `linear_qnet.py`, define the initialization and forward pass of a Q-network with a single linear layer which takes the state as input and outputs the Q-values for all actions.\n",
    "    - **Setting up Q-Learning**: In `core/dqn_train.py`, complete the functions `process_state`, `forward_loss` and `update_step` and `update_target_params`. The loss function for our Q-Networks is defined for a single transition tuple of (state, action, reward, next state) as follows. $Q(s_t, a_t)$ refers to the state-action values computed by our first Q-network at the current state and and for the current actions, $Q_{target}(s_{t+1}, a_{t+1})$ refers to the state-action values for the next state and all possible future actions computed by the target Q-Network\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Q_{sample}(s_t) &= r_t \\textrm{ if done} \\hspace{10cm}\\\\\n",
    "    &= r_t + \\gamma \\max_{a_{t+1}} Q_{target}\\left(s_{t+1}, a_{t+1}\\right) \\textrm{ otherwise}\\\\\n",
    "    \\textrm{Loss} &= \\left( Q_{sample}(s_t) - Q(s_t, a_t) \\right) ^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Deliverable 1 (15 points)\n",
    "\n",
    "Run the following block of code to train a Linear Q-Network. You should get an average reward of ~4.0, full credit will be given if average reward at the final evaluation is above 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: -1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1001/10000 [==>...........................] - ETA: 13s - Loss: 3.8926 - Avg_R: 0.6650 - Max_R: 2.0000 - eps: 0.8020 - Grads: 12.9740 - Max_Q: 0.8200 - lr: 0.0042 ETA: 11s - Loss: 1.0115 - Avg_R: 0.4400 - Max_R: 2.1000 - eps: 0.9208 - Grads: 5.3043 - Max_Q: 0.4004 - lr: 0 - ETA: 13s - Loss: 10.8250 - Avg_R: 0.3350 - Max_R: 3.0000 - eps: 0.8614 - Grads: 22.6048 - Max_Q: 0.6580 - lr: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.80 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2001/10000 [=====>........................] - ETA: 13s - Loss: 12.6671 - Avg_R: 1.4500 - Max_R: 4.0000 - eps: 0.6238 - Grads: 32.2130 - Max_Q: 1.7724 - lr: 0.003 - ETA: 13s - Loss: 12.1808 - Avg_R: 1.1550 - Max_R: 3.8000 - eps: 0.6040 - Grads: 24.8829 - Max_Q: 1.8560 - lr: 0.0034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3001/10000 [========>.....................] - ETA: 11s - Loss: 13.5121 - Avg_R: 1.9500 - Max_R: 4.1000 - eps: 0.4060 - Grads: 26.9340 - Max_Q: 2.5087 - lr: 0.0026- ETA: 13s - Loss: 11.4555 - Avg_R: 1.7400 - Max_R: 3.8000 - eps: 0.5644 - Grads: 25.4581 - Max_Q: 2.0379 - lr: 0.00 - ETA: 12s - Loss: 15.0758 - Avg_R: 1.5700 - Max_R: 4.1000 - eps: 0.5248 - Grads: 29.2509 - Max_Q: 2.1887 - lr: 0.003 - ETA: 12s - Loss: 7.5629 - Avg_R: 1.4200 - Max_R: 4.0000 - eps: 0.5050 - Grads: 23.9891 - Max_Q: 2.2549 - lr: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4001/10000 [===========>..................] - ETA: 10s - Loss: 16.1194 - Avg_R: 2.9150 - Max_R: 4.1000 - eps: 0.2080 - Grads: 25.7422 - Max_Q: 2.5906 - lr: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5001/10000 [==============>...............] - ETA: 8s - Loss: 3.4609 - Avg_R: 3.9850 - Max_R: 4.1000 - eps: 0.0100 - Grads: 12.1087 - Max_Q: 2.6064 - lr: 0.0010 - ETA: 10s - Loss: 9.0879 - Avg_R: 3.1500 - Max_R: 4.1000 - eps: 0.1684 - Grads: 23.9739 - Max_Q: 2.5935 - lr: - ETA: 9s - Loss: 2.3762 - Avg_R: 3.8700 - Max_R: 4.1000 - eps: 0.0892 - Grads: 13.5562 - Max_Q: 2.6092 - lr: 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6001/10000 [=================>............] - ETA: 6s - Loss: 0.2180 - Avg_R: 4.0500 - Max_R: 4.1000 - eps: 0.0100 - Grads: 4.8482 - Max_Q: 2.9011 - lr: 0.0010- ETA: 7s - Loss: 3.0691 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 9.2268 - Max_Q: 2.7857 - lr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7001/10000 [====================>.........] - ETA: 5s - Loss: 1.6149 - Avg_R: 4.0950 - Max_R: 4.1000 - eps: 0.0100 - Grads: 10.1882 - Max_Q: 2.6138 - lr: 0.001 - ETA: 5s - Loss: 0.4080 - Avg_R: 4.0100 - Max_R: 4.1000 - eps: 0.0100 - Grads: 11.1709 - Max_Q: 2.6117 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 3.80 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 8001/10000 [=======================>......] - ETA: 3s - Loss: 0.0142 - Avg_R: 4.0050 - Max_R: 4.1000 - eps: 0.0100 - Grads: 2.3713 - Max_Q: 2.7571 - lr: 0.0010 - ETA: 4s - Loss: 0.2103 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 6.0000 - Max_Q: 2.6040 - l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9001/10000 [==========================>...] - ETA: 1s - Loss: 0.2791 - Avg_R: 4.1000 - Max_R: 4.1000 - eps: 0.0100 - Grads: 3.5885 - Max_Q: 2.7440 - lr: 0.0010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10001/10000 [==============================] - ETA: 0s - Loss: 0.3712 - Avg_R: 4.0950 - Max_R: 4.1000 - eps: 0.0100 - Grads: 9.1129 - Max_Q: 2.5361 - lr: 0.00 - 17s - Loss: 0.0600 - Avg_R: 4.0350 - Max_R: 4.1000 - eps: 0.0100 - Grads: 1.8517 - Max_Q: 2.5327 - lr: 0.0010    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Training done.\n",
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.p1_linear import config as config_lin\n",
    "\n",
    "env = EnvTest((5, 5, 1))\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_lin.eps_begin,\n",
    "        config_lin.eps_end, config_lin.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_lin.lr_begin, config_lin.lr_end,\n",
    "        config_lin.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(LinearQNet, env, config_lin, device)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a final average reward of over 4.0 on the test environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning with Deep Q-Networks\n",
    "\n",
    "In `cnn_qnet.py`, implement the initialization and forward pass of a convolutional Q-network with architecture as described in this DeepMind paper:\n",
    "    \n",
    "\"Playing Atari with Deep Reinforcement Learning\", Mnih et. al. (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "### Deliverable 2 (10 points)\n",
    "\n",
    "Run the following block of code to train our Deep Q-Network. You should get an average reward of ~4.0, full credit will be given if average reward at the final evaluation is above 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: -0.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the memory 150/200..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: -0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 301/1000 [========>.....................] - ETA: 2s - Loss: 1.3270 - Avg_R: 0.1000 - Max_R: 2.8000 - eps: 0.4060 - Grads: 17.3416 - Max_Q: 0.0377 - lr: 0.0002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: -0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 401/1000 [===========>..................] - ETA: 2s - Loss: 2.2181 - Avg_R: 0.5150 - Max_R: 2.0000 - eps: 0.2080 - Grads: 42.4171 - Max_Q: 0.0895 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 501/1000 [==============>...............] - ETA: 2s - Loss: 1.2275 - Avg_R: 0.5400 - Max_R: 2.0000 - eps: 0.0100 - Grads: 48.6401 - Max_Q: 0.1099 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 0.50 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 601/1000 [=================>............] - ETA: 2s - Loss: 5.6025 - Avg_R: 0.4500 - Max_R: 0.5000 - eps: 0.0100 - Grads: 47.5985 - Max_Q: 0.1239 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 701/1000 [====================>.........] - ETA: 1s - Loss: 3.1406 - Avg_R: 3.8050 - Max_R: 4.0000 - eps: 0.0100 - Grads: 30.1634 - Max_Q: 0.2419 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 3.90 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 801/1000 [=======================>......] - ETA: 1s - Loss: 0.9577 - Avg_R: 3.8450 - Max_R: 4.1000 - eps: 0.0100 - Grads: 20.7538 - Max_Q: 0.3436 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 901/1000 [==========================>...] - ETA: 0s - Loss: 2.5619 - Avg_R: 3.6300 - Max_R: 4.1000 - eps: 0.0100 - Grads: 53.5469 - Max_Q: 0.4259 - lr: 0.0001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Average reward: 4.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1001/1000 [==============================] - 5s - Loss: 1.3149 - Avg_R: 3.8500 - Max_R: 4.1000 - eps: 0.0100 - Grads: 144.1170 - Max_Q: 0.4986 - lr: 0.0001    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Training done.\n",
      "Evaluating...\n",
      "Average reward: 4.10 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.p2_cnn import config as config_cnn\n",
    "\n",
    "env = EnvTest((80, 80, 1))\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_cnn.eps_begin,\n",
    "        config_cnn.eps_end, config_cnn.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_cnn.lr_begin, config_cnn.lr_end,\n",
    "        config_cnn.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(ConvQNet, env, config_cnn, device)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a final average reward of over 4.0 on the test environment, similar to the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Playing Atari Games from Pixels - using Linear Function Approximation\n",
    "\n",
    "Now that we have setup our Q-Learning algorithm and tested it on a simple test environment, we will shift to a harder environment - an Atari 2600 game from OpenAI Gym: Pong-v0 (https://gym.openai.com/envs/Pong-v0/), where we will use RGB images of the game screen as our observations for state.\n",
    "\n",
    "No additional implementation is required for this part, just run the block of code below (will take around 1 hour to train). We don't expect a simple linear Q-network to do well on such a hard environment - full credit will be given simply for running the training to completion irrespective of the final average reward obtained.\n",
    "\n",
    "You may edit `configs/p3_train_atari_linear.py` if you wish to play around with hyperparamters for improving performance of the linear Q-network on Pong-v0, or try another Atari environment by changing the `env_name` hyperparameter. The list of all Gym Atari environments are available here: https://gym.openai.com/envs/#atari\n",
    "\n",
    "### Deliverable 3 (5 points)\n",
    "\n",
    "Run the following block of code to train a linear Q-network on Atari Pong-v0. We don't expect the linear Q-Network to learn anything meaingful so full credit will be given for simply running this training to completion (without errors), irrespective of the final average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from configs.p3_train_atari_linear import config as config_lina\n",
    "\n",
    "# make env\n",
    "env = gym.make(config_lina.env_name)\n",
    "env = MaxAndSkipEnv(env, skip=config_lina.skip_frame)\n",
    "env = PreproWrapper(env, prepro=greyscale, shape=(80, 80, 1),\n",
    "                    overwrite_render=config_lina.overwrite_render)\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_lina.eps_begin,\n",
    "        config_lina.eps_end, config_lina.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_lina.lr_begin, config_lina.lr_end,\n",
    "        config_lina.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(LinearQNet, env, config_lina, device)\n",
    "print(\"Linear Q-Net Architecture:\\n\", model.q_net)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating...\n",
    "\n",
    "Linear Q-Net Architecture:\n",
    " LinearQNet(\n",
    "  (fc): Linear(in_features=25600, out_features=6, bias=True)\n",
    ")\n",
    "\n",
    "Average reward: -20.90 +/- 0.04\n",
    "250101/500000 [==============>...............] - ETA: 1126s - Loss: 4.0227 - Avg_R: -20.4200 - Max_R: -18.0000 - eps: 0.7749 - Grads: 402.3748 - Max_Q: 7.3714 - lr: 0.0001Evaluating...\n",
    "\n",
    "Average reward: -20.52 +/- 0.09\n",
    "500001/500000 [==============================] - 2388s - Loss: 1.8215 - Avg_R: -20.4000 - Max_R: -18.0000 - eps: 0.5500 - Grads: 347.0506 - Max_Q: 8.3961 - lr: 0.0001  - Training done.\n",
    "Evaluating...\n",
    "\n",
    "Average reward: -21.00 +/- 0.00\n",
    "\n",
    "#### NOTE: To make this code run faster, I used Google Colab. Above is the raw text I copy-pasted from the Colab notebook. Below is a screenshot of the results, which contains the same information. I just didn't want to waste my time running it all on my local machine.\n",
    "\n",
    "![Part-3-Training](part3-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: \\[BONUS\\] Playing Atari Games from Pixels - using Deep Q-Networks\n",
    "\n",
    "This part is extra credit and worth 5 bonus points. We will now train our deep Q-Network from Part 2 on Pong-v0. \n",
    "\n",
    "Again, no additional implementation is required but you may wish to tweak your CNN architecture in `cnn_qnet.py` and hyperparameters in `configs/p4_train_atari_cnn.py` (however, evaluation will be considered at no farther than the default 5 million steps, so you are not allowed to train for longer). Please note that this training may take a very long time (we tested this on a single GPU and it took around 6 hours).\n",
    "\n",
    "The bonus points for this question will be allotted based on the best evaluation average reward (EAR) before 5 million time stpes:\n",
    "\n",
    "1. EAR >= 0.0 : 4/4 points\n",
    "2. EAR >= -5.0 : 3/4 points\n",
    "3. EAR >= -10.0 : 3/4 points\n",
    "4. EAR >= -15.0 : 1/4 points\n",
    "\n",
    "### Deliverable 4: (5 bonus points)\n",
    "\n",
    "Run the following block of code to train your DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from configs.p4_train_atari_cnn import config as config_cnna\n",
    "\n",
    "\n",
    "# make env\n",
    "env = gym.make(config_cnna.env_name)\n",
    "env = MaxAndSkipEnv(env, skip=config_cnna.skip_frame)\n",
    "env = PreproWrapper(env, prepro=greyscale, shape=(80, 80, 1),\n",
    "                    overwrite_render=config_cnna.overwrite_render)\n",
    "\n",
    "# exploration strategy\n",
    "exp_schedule = LinearExploration(env, config_cnna.eps_begin,\n",
    "        config_cnna.eps_end, config_cnna.eps_nsteps)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_schedule  = LinearSchedule(config_cnna.lr_begin, config_cnna.lr_end,\n",
    "        config_cnna.lr_nsteps)\n",
    "\n",
    "# train model\n",
    "model = DQNTrain(ConvQNet, env, config_cnna, device)\n",
    "print(\"CNN Q-Net Architecture:\\n\", model.q_net)\n",
    "model.run(exp_schedule, lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: To make this code run faster, I used Google Colab. Below is a screenshot of the results. I just didn't want to waste my time running it all on my local machine. You can see the max reward achieved is -14.0\n",
    "\n",
    "![Part-4-Training](part4-best-training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
